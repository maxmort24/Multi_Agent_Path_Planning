# Multi-Agent Path Planning

This repository contains the source code for a term project on path planning in dynamic, multi-agent environments.

It explores the implementation and comparison of several search algorithms:
- **Greedy Best-First Search (BFS)**
- **A***
- **Weighted A***
- **Simplified Memory-Bounded A\* (SMA\*)**

Multiple heuristic strategies are applied to evaluate their efficiency.

The simulation features:
- Real-time agent communication under **partial observability**
- Performance benchmarking against paths generated by **Gemini's API**

## Setup Instructions

### Prerequisites
- Python 3.7+
- Internet connection (for Gemini API)

### Installation
1. Install required dependencies:
```bash
pip install -r requirements.txt
```

2. Set up Gemini API key:
   - Get your API key from [Google AI Studio](https://makersuite.google.com/app/apikey)
   - Create a `.env` file in the project root:
```bash
echo "GEMINI_API_KEY=your_api_key_here" > .env
```

## Quick Start

To quickly test the system:
```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Set up API key (get from https://makersuite.google.com/app/apikey)
echo "GEMINI_API_KEY=your_api_key_here" > .env

# 3. Create test environment
python create_environment.py
# Enter: 5 rows, 5 columns, 1 robot

# 4. Run basic test
python run_search.py --file robot_room.txt --include-gemini

# 5. Run detailed comparison for analysis
python gemini_comparison.py
```

## Detailed Testing Instructions

### Step 1: Create Test Environment
Create a grid-based environment for testing:
```bash
python create_environment.py
```
**Purpose**: Generates a `robot_room.txt` file with obstacles, robot positions, and goal location.
**Input**: You'll be prompted for:
- Number of rows (recommended: 5-30)
- Number of columns (recommended: 5-30) 
- Number of robots (1-4)

**Recommended Test Scenarios**:
- **Simple**: 5 rows, 5 columns, 1 robot (good for initial testing)
- **Medium**: 8 rows, 10 columns, 2 robots (shows performance differences)
- **Complex**: 15+ rows, 15+ columns, 3-4 robots (stress testing)

### Step 2: Basic Algorithm Testing
Test traditional pathfinding algorithms:
```bash
python run_search.py --file robot_room.txt
```
**Purpose**: Runs A*, Greedy BFS, and Weighted A* algorithms with visualization.
**Expected Output**: 
- Path visualizations for each algorithm
- Path lengths and costs
- Visual grid showing found paths

### Step 3: Test with Gemini API Integration
Include AI-generated paths in comparison:
```bash
python run_search.py --file robot_room.txt --include-gemini
```
**Purpose**: Adds Gemini API pathfinding to the traditional algorithm tests.
**Expected Output**: Same as Step 2, plus Gemini API results (if API key is configured).

### Step 4: Simple Comparison Mode
Run basic comparison for assignment data collection:
```bash
python run_search.py --compare --file robot_room.txt
```
**Purpose**: Basic comparison between algorithms and Gemini API.
**Expected Output**:
- Simple success/failure status for each algorithm
- Basic performance comparison
- Recommendation to use detailed comparison script

### Step 5: Detailed Comparison for Analysis
Generate data for Reporting purposes:
```bash
python gemini_comparison.py
```
**Purpose**: Collects detailed comparison data for Algorithm analysis.
**Expected Output**:
- Execution time measurements
- Path length comparisons
- Success rate analysis
- JSON file with raw data for charts/tables
- Summary suitable for assignment documentation

## Command Reference

### Basic Commands
| Command | Purpose |
|---------|---------|
| `python create_environment.py` | Create test environment file |
| `python run_search.py --file <filename>` | Run basic pathfinding algorithms |
| `python run_search.py --include-gemini` | Include Gemini API in basic testing |

### Comparison Commands
| Command | Purpose |
|---------|---------|
| `python run_search.py --compare` | Basic comparison between algorithms |
| `python gemini_comparison.py` | Detailed comparison for algorithm analysis |

### Advanced Options
| Option | Description |
|--------|-------------|
| `--algorithms "A* Manhattan,Greedy BFS Manhattan"` | Specify which algorithms to test |
| `--file custom_environment.txt` | Use custom environment file |
| `--output results_directory` | Specify output directory for reports |
| `--max-retries 5` | Set maximum API retry attempts |
| `--timeout 60` | Set API timeout in seconds |

## Verification and Expected Results

### What to Look For:
1. **Algorithm Correctness**:
   - A* should find optimal paths (shortest length)
   - All paths should avoid obstacles
   - Paths should connect start to goal positions

2. **Performance Metrics**:
   - Execution times should be < 1ms for simple grids
   - A* typically slower but more optimal than Greedy BFS
   - Weighted A* should balance speed and optimality

3. **Gemini API Integration**:
   - May fail if API key not configured
   - Higher latency (1000-5000ms) due to network calls
   - Variable path quality depending on AI interpretation

### Troubleshooting:
- **"Environment file not found"**: Run `create_environment.py` first
- **"Gemini API could not find a path"**: Check API key in `.env` file
- **Import errors**: Install dependencies with `pip install -r requirements.txt`

## Output Files
The comprehensive analysis generates:
- **JSON Report**: Machine-readable performance data
- **Text Summary**: Human-readable analysis with recommendations  
- **Raw Results**: Detailed execution data for further analysis

All output files are timestamped and saved in the `comparison_results/` directory.

## File Structure

```
Multi_Agent_Path_Planning/
├── create_environment.py             # Environment generation utility
├── interpret_environment.py          # Environment file parser
├── search_algorithms.py              # Core pathfinding algorithms
├── robot_partial_robots.py           # Partial knowledge robot
├── run_partial_knowledge.py          # Partial knowledge robot implementation
├── gemini_api_client.py              # Gemini API integration
├── gemini_comparison.py              # Comparison for algorithm comparison
└── grid_renderer.py                  # Visualization functions
```

## Expected Performance Benchmarks

Based on testing with different grid sizes:
Note: Unsuccessful runs typically result from robots enclosed by obstacles

### Simple Grid (5x5, 1 robot):
- **A* Manhattan**: ~0.08ms, 100% success, optimal paths
- **A* Euclidean**: ~0.04ms, 100% success, optimal paths  
- **Greedy BFS**: ~0.04ms, 100% success, usually optimal
- **Weighted A***: ~0.04ms, 100% success, optimal paths
- **Gemini API**: ~550ms, 100% success, optimal paths

### Medium Grid (8x10, 2 robots):
- **Manual Algorithms**: 0.05-0.10ms, 100% success
- **Gemini API**: 500-1000ms, variable success (depends on complexity)

### Complex Grid (15x15, 3+ robots):
- **Manual Algorithms**: 0.1-1.0ms, high success rate
- **Gemini API**: 1000-3000ms, variable performance
